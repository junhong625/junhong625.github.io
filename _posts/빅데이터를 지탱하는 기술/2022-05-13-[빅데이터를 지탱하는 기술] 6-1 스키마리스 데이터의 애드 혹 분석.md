---
layout : post
title : '[빅데이터를 지탱하는 기술] 6-1 스마리스 데이터의 애드 혹 분석'
subtitle : '[빅데이터를 지탱하는 기술] 정리'
gh-repo: junhong625/Study
gh-badge: [star, fork, follow]
tags : [빅데이터를 지탱하는 기술, Study]
comments: true
---

# 6-1 스마리스 데이터의 애드 혹 분석
- - - 

## 스키마리스 데이터 수집하기
- Tiwtter의 스트리밍 API를 이용하여 트윗 데이터를 수집 [[https://developer.twitter.com/en]](https://developer.twitter.com/en)
Twitter에 흘러가는 트윗을 분석하는 실습을 진행한다.  
데이터를 '스트리밍 API'를 사용하여 샘플링된 데이터를 실시간으로 수집하고 'MongoDB'에 보관한다.  
각 트윗은 들쭉날쭉한 JSON 데이터로 되어 있는데 이 데이터를 대화식으로 가공해 시각화해본다.  

간단하게 인증을 받고 4가지의 토큰을 받아둔다.
- consumer_key
- consumer_secret
- access_token_key
- access_token_secret
<br>
<br>

## 테스트 환경의 구축
- JDK 설치확인(기존에 설치해뒀던 java가 있다.)
- 파이썬의 실행 환경을 준비
- MongoDB 설치
<br>
<br>

### 트위치 스트리밍 데이터 수집
여기서 문제가 하나 발생한다.  
책에 나와있는 리스트 6.2 [Twitter 스트리밍 API를 호출하는 스크립트(twitter-streaming.py)]를 보면 v1.1 스트리밍 엔드포인트에 접속을 시도하는데  
이 부분에서 **http 403** 에러가 발생해 이 에러를 해결하기 위해 1일 동안 구글링하며 많은 시간을 소모하게 된다.  
Access권한을 Essential에서 Elevated로 업그레이도 하고 시도도 해봤고 정말 다양한 시도를 해봤다.  
그래도 해결이 되지 않던 중 [twittercommunity](https://twittercommunity.com/)에 '403'을 검색해봤더니 '최근 발표에 따르면 새로운 개발자 계정은 기존의 v1.1 스트리밍 엔드포인트가 아닌 v2 스트리밍 엔드포인트를 사용해야 한다.'는 글을 찾을 수 있었다.  
그리하여 기존의 책에 있던 코드를 변경하여 새롭게 코드를 짜 보았다.  
``` python
import requests
import os
import json
import datetime
from pymongo import MongoClient

client = MongoClient('localhost', 27017)  # Mongodb에 연결합니다.
client.drop_database('twitter')
db = client.twitter  # twitter라는 데이터베이스 만듭니다.

def create_url():
    return "https://api.twitter.com/2/tweets/sample/stream?tweet.fields=lang,created_at"

def create_headers(bearer_token):
    headers = {"Authorization": "Bearer {}".format(bearer_token)}
    return headers

def connect_to_endpoint(url, headers):
    response = requests.request("GET", url, headers=headers, stream=True)
    print(response.status_code)
    for response_line in response.iter_lines():
        if response_line:
            json_response = json.loads(response_line)
            ################## 바뀐 부분 ####################
            json_response['_timestamp'] = datetime.datetime.utcnow().isoformat()
            db.sample.insert_one(json_response)
            #########################################

    if response.status_code != 200:
        raise Exception(
            "Request returned an error: {} {}".format(
                response.status_code, response.text
            )
        )

def main():
    bearer_token = '********'
    url = create_url()
    headers = create_headers(bearer_token)
    timeout = 0
    while True:
        connect_to_endpoint(url, headers)
        timeout += 1


if __name__ == "__main__":
    main()
```
<br>
<br>

## 대화식 실행 환경의 준비
- Jupyter Notebook에서 MongoDB의 내용 확인하기
    - created_at, text의 데이터만 뽑아내 데이터프레임형식으로 만든다.

```python
## v2 스트리밍 엔드포인트에 맞게 코드를 변환

import pymongo
import pandas as pd

mongo = pymongo.MongoClient('localhost', 27017)
db = mongo.twitter
collection = db.sample

def tweets(args, **kwargs):
    tweets = list(mongo.twitter.sample.find(**kwargs))
    category = str(args).split("'")[1]
    args = str(args).split("'")[3]
    for tweet in tweets:
        if args in tweet['data'][category]:
            yield {
                'created_at': tweet['data']['created_at'],
                'text': tweet['data']['text'],
            }

pd.DataFrame(tweets({'lang': 'en'}))
```

> 출력 결과
<img height=300 src="https://user-images.githubusercontent.com/83000975/172341328-19954c5f-3b1c-403a-b28d-54d8d16c7349.png">
<br>
<br>

## Spark에 의한 분산 환경 - 데이터양이 늘어도 대응 가능하게 하기
- 클라이언트 : **드라이버 프로그램(driver program)**
- pyspark를 실행하여 파이썬에서 대화식의 Spark를 실행
    - Spark : 마스터/슬레이브 형의 분산 시스템으로 클라이언트로부터 마스터에 명령을 보냄으로써 프로그램을 실행
    - 멀티 코어를 활용해 병렬 처리
- 드라이버 프로그램을 주피터와 조합시키면 대화식 데이터 처리를 실행이 수월
- apache spark 설치 후 환경변수 설정

```zsh
vi ~/.bash_profile # 환경변수 열기

# .bash_profile에 추가
export JAVA_HOME=/Libary/Java/JavaVirtualMachines/<자바 폴더>
export SPARK_PATH=<Spark 설치 경로>
export PYSPARK_DRIVER_PYTHON="jupyter-console"
export PYSPARK_DRIVER_PYTHON_OPTS="notebook"
export PYSPARK_PYTHON=<내 파이썬 버전>
export PYSPARK_SUBMIT_ARGS="--master local[2] pyspark-shell"
```

- Spark를 기동하고 MongoDB 용의 패키지 추가

```zsh
# Spark, MongoDB, mongo-spark-connector 의 새로운 버전에 맞게 변경

pyspark --conf "spark.mongodb.read.connection.uri=mongodb://127.0.0.1/twitter.sample?readPreference=primaryPreferred" \
        --conf "spark.mongodb.write.connection.uri=mongodb://127.0.0.1/tiwtter.sample" \
        --packages org.mongodb.spark:mongo-spark-connector:10.0.2
```
<br>
<br>

### MongoDB의 애드 혹 집계
- MongoDB로부터 데이터 읽어들이기 위해 Spark 세션을 통해 **데이터 프레임**을 작성

``` zsh
# MongoDB 4.4.13 Community의 환경에 맞게 코드 변경
df = spark.read.format("mongodb").option("uri", "mongodb://127.0.0.1/twitter.sample").load()
```

- Spark SQL을 사용하여 데이터 프레임을 SQL로 집계
    - MongoDB의 경우 열 지향 스토리지가 아니기에 고속 집계에는 적합하지 않다.
<br>
<br>

### 텍스트 데이터의 가공 - 스크립트 언어의 활용
- Spark의 최대 장점 : 스크립트 언어에 의한 프로그래밍  

- query로 Spark 데이터 프레임 생성 ➡︎ 제네레이터 함수 생성 ➡︎ '.rdd'로 원시 레코드 참조 ➡︎ flatMap()에 제네레이터 함수 적용 ➡︎ toDF()를 사용해 데이터 프레임으로 변환
<br>
<br>

### spark 프로그램에 있어서의 DAG 실행
- Spark는 데이터 프레임을 토대로 **RDD(Resilient Distributed Dataset)**라 불리는 로우 레벨의 데이터 구조로 되어 있다.
    - MapReduce와 마찬가지로 Map과 Reduce로 적용 가능
<img height=300 src="https://user-images.githubusercontent.com/83000975/172998287-dd18436d-3c67-4f2a-a3cd-c2663b4e1f62.jpeg">
<br>
<br>

## 데이터를 집계해서 데이터 마트 구축하기
- 시각화에 적합한 데이터 마트 구축 선택지 3가지
    - Spark에 ODBC/JDBC로 접속하기
    - MPP 데이터베이스에 비정규화 테이블 만들기
    - 데이터를 작게 집약하여 CSV 파일에 출력하기  

- 카디널리티 삭감 : 시각화 프로세스에 효과가 있다.  

- CSV 파일의 작성 방법 : spark-csv 라이브러리, pandas의 데이터 프레임
<br>
<br>

## BI 도구로 데이터 시각화 하기
- BI 도구는 스프레드시트보다 더 많은 레코드 수에도 대응이 가능하다.  

- 가공하지 않은 데이터를 시각화 하기보다 노트북을 통해 시각화 하기 쉬운 데이터를 만들고 BI 도구로 읽어들여 시각화 처리하는 작업을 반복적으로 하는 것이 생산적이다.  

- BI 도구 유형 : 데스크톱 형 BI 도구, 웹 형 BI 도구