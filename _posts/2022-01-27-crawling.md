---
layout : post
title : '[Python]크롤링으로 서울시 부동산 중개업 데이터 가져오기'
subtitle : crawling
gh-repo: junhong625/A_FIT
gh-badge: [star, fork, follow]
tags : [Python]
comments: true
---
  
Selenium을 활용한 크롤링 실습을 위해 [[국가공간정보포털](http://www.nsdi.go.kr/lxportal/?menuno=4085)]에서 서울시 부동산중개업 데이터를 추출하여 저장하는 자동화 코드를 작성해봤습니다. 
본 포스팅은 해당 코드를 정리한 것입니다.

## 크롤링 환경 세팅
> 기본적으로 파이썬이 설치되어 있는 환경

**1. 필요 라이브러리 설치**

크롤링을 원활히 할 수 있는 환경을 세팅해주기 위해 **'beautifulsoup4, requests, selenium'** 총 3가지를 설치할 것입니다.
```python
# beautifulsoup4 라이브러리 설치
pip install beautifulsoup4

# requests 라이브러리 설치
pip install requests

# selenium 라이브러리 설치
pip install selenium
```
무사히 설치를 모두 마쳤으면 코드를 작성해보겠습니다.

**2. import하기**  

필요한 기능들을 import해서 불러옵니다.
```python
import requests 
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.common.exceptions import NoSuchElementException, UnexpectedAlertPresentException 
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait as WDW
from selenium.webdriver.support import expected_conditions as EC
import time
```

**3. list 생성**   

데이터가 들어갈 list를 생성해줍니다.
```python
name_list = [] # '상호명'이 들어갈 list

address_list = [] # '소재지'가 들어갈 list

person_list = [] # '대표자'가 들어갈 list

phonenumber_list = [] # '전화번호'가 들어갈 list
```

**4. Chrome driver로 html페이지 가져오기**
```python
driver = webdriver.Chrome('chromedriver.exe') # ()안에 chromedriver.exe파일 경로 입력

driver.get('http://www.nsdi.go.kr/lxportal/?menuno=4085') # ()안에 접속할 url 입력
```

**5. 서울특별시와 각 구의 XPath 알아내기**
  
**개발자도구(F12)**를 통해 서울특별시와 각 구의 XPath를 알아냈습니다.

<img src='https://user-images.githubusercontent.com/83000975/152925195-b24850b6-e5b7-4fd1-884b-b470b5f75e9f.png' height=800 width=1200>

```python
서울특별시의 XPath : //*[@id="shSido"]/option[2]
각 구의 XPath : //*[@id="shSigungu"]/option[number] # number = 각 구의 넘버(2~27)
```

**6. 알아낸 XPath를 이용하여 카테고리 자동 선택**

**find_element_by_xpath** 함수를 활용하면 XPath를 이용하여 카테고리 선택이 가능합니다.
```python
driver.find_element_by_xpath('//*[@id="shSido"]/option[2]').click() # 서울특별시

driver.find_element_by_xpath('//*[@id="shSigungu"]/option[%s]'%(idx)).click() # 각 구
```
XPath 뿐만이 아니라 
- tag_name 
- id 
- class_name
- link_text
- partial_link_text
- css_selector

이와 같은 다른 요소로도 카테고리 선택이 가능합니다.

**7. 검색버튼 클릭**

위와 같은 방식으로 검색버튼의 **XPath**를 알아내 검색버튼이 자동으로 클릭되도록 코드를 작성합니다.
```python
driver.find_element_by_xpath('//*[@id="icon_btn_write"]').click() #검색버튼 클릭
```

**8. 현재 url에 HTML 파싱하기**
```python
full_html = driver.page_source # 현재 페이지 url full_html로 저장

soup = BeautifulSoup(full_html, 'html.parser') # 현재 주소 파싱
```

**9. BeautifulSoup4의 select_one함수를 이용하여 해당구의 부동산 수 가져오기**

**select_one**함수와 **get_text**함수를 이용한다면 쉽게 원하는 텍스트를 가져올 수 있습니다.
```python
# select_one함수에 내가 원하는 부분의 selector 경로를 집어넣고 get_text함수를 통해 total에 해당구의 부동산 수 텍스트 저장
total = soup.select_one('#searchVO > div.bl_table > div.global-btn-wrap > p > span').get_text()
```

**10. total에 페이지 수 저장**

해당 구의 페이지 수만큼 반복하기 위해 **total**에 조건문을 거쳐 해당 구의 페이지 수를 저장합니다.
```python
# 만약 total(부동산 수)가 천단위로 넘어간다면 ','가 포함되기 때문에 split을 통해 ','제거합니다.
if len(total) > 3:
    total = int(total.split(',')[0] + total.split(',')[1])

# 10으로 딱 나누어 떨어진다면 페이지 넘버가 10으로 나누어진 수만큼 있을 것이기에 아무것도 더해주지 않습니다.
if int(total) % 10 == 0:
    total = int(total)//10

# 하지만 10으로 딱 나누어 떨어지지 않는다면 페이지 넘버가 1더 있을 것이기에 total에 1을 더해줍니다.
else:
    total = int(total)//10 + 1
```
   

**11. length에 상호명 수를 저장**

부동산의 상호명을 클릭하면 해당 부동산의 데이터가 저장되어있는 경로로 이동되는데 페이지에 있는 상호명 수 만큼 반복하기 위해 **length**에 링크 수를 저장합니다. 
```python
# select_one은 제일 처음에 나오는 해당 selector의 데이터를 가져오기에 select_one이 아닌 select를 이용하여 모든 링크를 가져와 링크들의 수를 length에 저장합니다.
length = len(soup.select('.t_l > a')) 
```

**11-1. length 수정**

위의 코드로 진행했더니 휴업 또는 정지상태인 부동산은 상호명이 링크로 설정이 되어있지 않기에 링크 수 만큼 반복을 하면 10번이 반복되지 않아 휴업이나 정지 상태인 부동산을 건너 뛰는 경우가 발생하게 됩니다.
모든 부동산의 데이터를 추출하기 위해 length에 링크 수가 아닌 상호명 수가 저장되도록 수정하였습니다.
```python
# 'a'가 링크를 나타내는 스타일이기에 'a'를 빼고 해당 페이지 html의 '.t_l'클래스를 불러옵니다. 
# 't_l'클래스는 각 부동산의 [상호, 소재지, 대표자] 총 세 종류의 데이터를 가져오기에 3을 나눠주어 해당 페이지의 부동산 수를 length에 집어넣어 줍니다. 
length = len(soup.select('.t_l'))//3
```

**11-2. try, except 예외 설정**
